{
  
    
        "post0": {
            "title": "KERRY MATRIX",
            "content": "#collapse #IMPORTS import pandas as pd from matplotlib import pyplot as plt rtData = pd.read_csv(&#39;https://d14wlfuexuxgcm.cloudfront.net/covid/rt.csv&#39;) dailyData = pd.read_csv(&#39;https://covidtracking.com/api/v1/states/daily.csv&#39;) . . #collapse #SET VARIABLES today = &#39;2020-08-02&#39; week = &#39;2020-08-02/2020-07-26&#39; dateRange = &#39;20200802&#39; . . #collapse #KERRY LOCATIONS AND POPULATION DATA kerryLocations = pd.read_csv(&#39;kerry.csv&#39;) kerryLocations.columns = map(str.lower, kerryLocations.columns) kerryCols=[&#39;state&#39;, &#39;population&#39;] kerryLocs = kerryLocations[kerryCols] . . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-3-8cab67a2f69d&gt; in &lt;module&gt; 1 #collapse 2 #KERRY LOCATIONS AND POPULATION DATA -&gt; 3 kerryLocations = pd.read_csv(&#39;kerry.csv&#39;) 4 kerryLocations.columns = map(str.lower, kerryLocations.columns) 5 kerryCols=[&#39;state&#39;, &#39;population&#39;] C: Users anaconda3 envs USAcovidMAP lib site-packages pandas io parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision) 674 ) 675 --&gt; 676 return _read(filepath_or_buffer, kwds) 677 678 parser_f.__name__ = name C: Users anaconda3 envs USAcovidMAP lib site-packages pandas io parsers.py in _read(filepath_or_buffer, kwds) 446 447 # Create the parser. --&gt; 448 parser = TextFileReader(fp_or_buf, **kwds) 449 450 if chunksize or iterator: C: Users anaconda3 envs USAcovidMAP lib site-packages pandas io parsers.py in __init__(self, f, engine, **kwds) 878 self.options[&#34;has_index_names&#34;] = kwds[&#34;has_index_names&#34;] 879 --&gt; 880 self._make_engine(self.engine) 881 882 def close(self): C: Users anaconda3 envs USAcovidMAP lib site-packages pandas io parsers.py in _make_engine(self, engine) 1112 def _make_engine(self, engine=&#34;c&#34;): 1113 if engine == &#34;c&#34;: -&gt; 1114 self._engine = CParserWrapper(self.f, **self.options) 1115 else: 1116 if engine == &#34;python&#34;: C: Users anaconda3 envs USAcovidMAP lib site-packages pandas io parsers.py in __init__(self, src, **kwds) 1889 kwds[&#34;usecols&#34;] = self.usecols 1890 -&gt; 1891 self._reader = parsers.TextReader(src, **kwds) 1892 self.unnamed_cols = self._reader.unnamed_cols 1893 pandas _libs parsers.pyx in pandas._libs.parsers.TextReader.__cinit__() pandas _libs parsers.pyx in pandas._libs.parsers.TextReader._setup_parser_source() FileNotFoundError: [Errno 2] File kerry.csv does not exist: &#39;kerry.csv&#39; . print (&#39;Kerry Locations Information being loaded...&#39;) kerryLocs . #collapse #rtlive data cleaning rtCleaned = rtData[rtData[&#39;date&#39;]==today] rtColumns = [&#39;date&#39;, &#39;region&#39;, &#39;mean&#39;] rtCleaned = rtCleaned[rtColumns] rtCleaned = rtCleaned.rename(columns = {&#39;region&#39;:&#39;state&#39;}) rtCleaned = rtCleaned.sort_values(&#39;state&#39;, ascending=True) #PRINT CLEANED DATA print (&#39;rtLive Data being loaded...&#39;) rtCleaned . . #collapse #DAILY DATA CLEANING dailyColumns = [&#39;date&#39;, &#39;state&#39;, &#39;death&#39;, &#39;positiveIncrease&#39;, &#39;totalTestResultsIncrease&#39;, &#39;deathIncrease&#39;] dailyData = dailyData[dailyColumns] dailyData = dailyData.sort_values(&#39;state&#39;, ascending=True) dailyCleaned = dailyData[dailyData[&#39;date&#39;].astype(str)==dateRange] dailyCleaned . . #collapse #FILTER DATASETS TO KERRY LOCATIONS AND MERGE SETS INTO ONE TABLE startFile=pd.merge(kerryLocs.state, rtCleaned, on=[&#39;state&#39;]) print (&#39;Kerry File Building...&#39;) startFile . . #collapse newData=pd.merge(startFile, dailyCleaned, on=[&#39;state&#39;]) newData=newData.rename(columns={&#39;date_x&#39;:&#39;date&#39;}) colss = [&#39;date&#39;, &#39;state&#39;, &#39;mean&#39;, &#39;death&#39;, &#39;positiveIncrease&#39;, &#39;totalTestResultsIncrease&#39;, &#39;deathIncrease&#39;] newData[colss] . . #collapse #ADDING CASES, TESTING, AND DEATHS TO DATA FILE dateRange=&#39;20200802&#39; cols = [&#39;date&#39;, &#39;state&#39;, &#39;death&#39;, &#39;positiveIncrease&#39;, &#39;totalTestResultsIncrease&#39;, &#39;deathIncrease&#39;] dailyData = dailyData[cols] dailyData = dailyData.sort_values(&#39;state&#39;, ascending=True) dailyData = dailyData[dailyData[&#39;date&#39;].astype(str)==dateRange] dailyData newData=pd.merge(startFile, dailyData, on=[&#39;state&#39;]) newData=newData.rename(columns={&#39;date_x&#39;:&#39;date&#39;}) colss = [&#39;date&#39;, &#39;state&#39;, &#39;mean&#39;, &#39;death&#39;, &#39;positiveIncrease&#39;, &#39;totalTestResultsIncrease&#39;, &#39;deathIncrease&#39;] newData[colss] . . #collapse startFile=pd.merge(kerryLocations.state, rtCleaned, on=[&#39;state&#39;]) print (&#39;Kerry File Building...&#39;) startFile . . #collapse read = pd.read_csv(&#39;us.csv&#39;) read.shape read.head() fee=[&#39;Kerry Locations&#39;, &#39;State&#39;, &#39;Population&#39;, &#39;SD Past&#39;] tre=read[fee] tre . . #collapse plt.title(&quot;Canada Matrix Chart&quot;) plt.xlabel(&quot;State&quot;) plt.ylabel(&quot;SD Past&quot;) x = tre[&quot;SD Past&quot;] y = tre[&quot;State&quot;] plt.plot(x,y) . . #collapse mer = pd.merge(rtChanged, dailyData, on=[&#39;state&#39;]) . . #collapse mer . . #collapse meg = pd.merge(kerryLocations, mer, on=&#39;state&#39;) meg = meg.rename(columns={&#39;date_x&#39;:&#39;date&#39;}) coll = [&#39;state&#39;, &#39;mean&#39;, &#39;death&#39;, &#39;positiveIncrease&#39;, &#39;totalTestResultsIncrease&#39;, &#39;deathIncrease&#39;] mew = meg[coll] print (&#39;Kerry Locations&#39;) print (&#39;Matrix Report&#39;) mew . . #collapse #chart over time fig, ax = plt.subplots(figsize=(10,5)) dailyData.death.plot(c=&quot;g&quot;, label=&quot;Test-adjusted&quot;) dailyData.positiveIncrease.plot(c=&quot;g&quot;, alpha=.5, label=&quot;Test-adjusted (raw)&quot;, style=&quot;--&quot;) dailyData.deathIncrease.plot(c=&quot;b&quot;, label=&quot;Infections&quot;) fig.set_facecolor(&#39;w&#39;) ax.legend(); . .",
            "url": "https://richcastro82.github.io/Matrix/jupyter/2020/08/05/USM.html",
            "relUrl": "/jupyter/2020/08/05/USM.html",
            "date": " • Aug 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "toc: false | comments: true | image: images/US_case_count.png | hide: false | search_exclude: false | categories: geospatial | author: Shantam Raj | badges: true | . Today we will make our first geospatial map from the article Coronavirus in the U.S.: Latest Map and Case Count which looks like the folowing - . import geopandas as gpd import altair as alt import pandas as pd alt.renderers.set_embed_options(actions=False) # Shapefiles from us census state_shpfile = &#39;./shapes/cb_2019_us_state_20m&#39; county_shpfile = &#39;./shapes/cb_2019_us_county_20m&#39; states = gpd.read_file(state_shpfile) county = gpd.read_file(county_shpfile) # Adding longitude and latitude in state data states[&#39;lon&#39;] = states[&#39;geometry&#39;].centroid.x states[&#39;lat&#39;] = states[&#39;geometry&#39;].centroid.y # Adding longitude and latitude in state data county[&#39;lon&#39;] = county[&#39;geometry&#39;].centroid.x county[&#39;lat&#39;] = county[&#39;geometry&#39;].centroid.y . # NYT dataset county_url = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&#39; cdf = pd.read_csv(county_url) . cdf[cdf[&#39;fips&#39;].isnull() == True].groupby([&#39;county&#39;]).sum() . fips cases deaths . county . Joplin 0.0 | 329 | 4 | . Kansas City 0.0 | 85094 | 1700 | . New York City 0.0 | 15615980 | 1528538 | . Unknown 0.0 | 885104 | 41064 | . #hide_output cdf[cdf[&#39;fips&#39;].isnull() == True].groupby([&#39;county&#39;, &#39;state&#39;]).sum() . NYT publishes the data for New York City in a different way by combining the results of the 5 boroughs that comprise it. So we will combine them too and add a new row in the dataset with a custom fips of 1. Let&#39;s start by making this change in the raw NYT dataset itself. . cdf.loc[cdf[&#39;county&#39;] == &#39;New York City&#39;,&#39;fips&#39;] = 1 cdf[cdf[&#39;county&#39;] == &#39;New York City&#39;] . date county state fips cases deaths . 416 2020-03-01 | New York City | New York | 1.0 | 1 | 0 | . 448 2020-03-02 | New York City | New York | 1.0 | 1 | 0 | . 482 2020-03-03 | New York City | New York | 1.0 | 2 | 0 | . 518 2020-03-04 | New York City | New York | 1.0 | 2 | 0 | . 565 2020-03-05 | New York City | New York | 1.0 | 4 | 0 | . ... ... | ... | ... | ... | ... | ... | . 262876 2020-06-23 | New York City | New York | 1.0 | 217803 | 21817 | . 265930 2020-06-24 | New York City | New York | 1.0 | 218089 | 21838 | . 268988 2020-06-25 | New York City | New York | 1.0 | 218429 | 21856 | . 272054 2020-06-26 | New York City | New York | 1.0 | 218799 | 21893 | . 275123 2020-06-27 | New York City | New York | 1.0 | 219157 | 21913 | . 119 rows × 6 columns . # collapse latest_cases = cdf.groupby(&#39;fips&#39;, as_index=False).agg({&#39;county&#39;: &#39;last&#39;, &#39;date&#39;: &#39;last&#39;, &#39;state&#39;: &#39;last&#39;, &#39;cases&#39;: &#39;last&#39;, &#39;deaths&#39;: &#39;last&#39;}) latest_cases . . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-27 | New York | 219157 | 21913 | . 1 1001.0 | Autauga | 2020-06-27 | Alabama | 498 | 12 | . 2 1003.0 | Baldwin | 2020-06-27 | Alabama | 555 | 10 | . 3 1005.0 | Barbour | 2020-06-27 | Alabama | 317 | 1 | . 4 1007.0 | Bibb | 2020-06-27 | Alabama | 161 | 1 | . ... ... | ... | ... | ... | ... | ... | . 3038 56037.0 | Sweetwater | 2020-06-27 | Wyoming | 81 | 0 | . 3039 56039.0 | Teton | 2020-06-27 | Wyoming | 119 | 1 | . 3040 56041.0 | Uinta | 2020-06-27 | Wyoming | 167 | 0 | . 3041 56043.0 | Washakie | 2020-06-27 | Wyoming | 38 | 5 | . 3042 56045.0 | Weston | 2020-06-27 | Wyoming | 1 | 0 | . 3043 rows × 6 columns . Now we have to make the changes in our shapefile too. For that we need to **dissolve** the 5 buroughs into one single geospatial entity. . #New York City fips = 36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085 which corresponds to New York, Kings, Queens, Bronx and Richmond spatial_nyc = county[county[&#39;GEOID&#39;].isin([&#39;36005&#39;, &#39;36047&#39;, &#39;36061&#39;, &#39;36081&#39;, &#39;36085&#39;])] . combined_nyc = spatial_nyc.dissolve(by=&#39;STATEFP&#39;) alt.Chart(spatial_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() | alt.Chart(combined_nyc).mark_geoshape(stroke=&#39;white&#39;, strokeWidth=3).encode() . agg_nyc_data = spatial_nyc.dissolve(by=&#39;STATEFP&#39;).reset_index() agg_nyc_data[&#39;GEOID&#39;] = &#39;1&#39; agg_nyc_data[&#39;fips&#39;] = 1 agg_nyc_data[&#39;lon&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.x agg_nyc_data[&#39;lat&#39;] = agg_nyc_data[&#39;geometry&#39;].centroid.y . agg_nyc_data . STATEFP geometry COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER lon lat fips . 0 36 | POLYGON ((-74.24921 40.54506, -74.21684 40.558... | 061 | 00974129 | 0500000US36061 | 1 | New York | 06 | 58690498 | 28541727 | -73.927011 | 40.695278 | 1 | . # hide_output county_nyc = gpd.GeoDataFrame(pd.concat([county, agg_nyc_data], ignore_index=True)) county_nyc[&#39;fips&#39;] = county_nyc[&#39;GEOID&#39;] county_nyc[&#39;fips&#39;] = county_nyc[&#39;fips&#39;].astype(&#39;int&#39;) county_nyc # generate FIPS in the shapefile itself by combining STATEFP and COUNTYFP #county2[&#39;STATEFP&#39;] + county2[&#39;COUNTYFP&#39;] #latest_cases[&#39;fips&#39;] = latest_cases[&#39;fips&#39;].astype(&#39;int&#39;) . latest_cases[&#39;fips&#39;].isin(county_nyc[&#39;fips&#39;]).value_counts() . True 3043 Name: fips, dtype: int64 . latest_cases[latest_cases[&#39;county&#39;] == &#39;New York City&#39;] . fips county date state cases deaths . 0 1.0 | New York City | 2020-06-27 | New York | 219157 | 21913 | . county_nyc[county_nyc[&#39;fips&#39;] == 1] . STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER geometry lon lat fips . 3220 36 | 061 | 00974129 | 0500000US36061 | 1 | New York | 06 | 58690498 | 28541727 | POLYGON ((-74.24921 40.54506, -74.21684 40.558... | -73.927011 | 40.695278 | 1 | . # collapse latest_cases_w_fips = county_nyc.merge(latest_cases, how=&#39;left&#39;, on=&#39;fips&#39;) circle_selection = alt.selection_single(on=&#39;mouseover&#39;, empty=&#39;none&#39;) circles = alt.Chart(latest_cases_w_fips).mark_point(fillOpacity=0.2, fill=&#39;red&#39;, strokeOpacity=1, color=&#39;red&#39;, strokeWidth=1).encode( latitude=&quot;lat:Q&quot;, longitude=&quot;lon:Q&quot;, size=alt.Size(&#39;cases:Q&#39;, scale=alt.Scale(domain=[0, 7000],),legend=alt.Legend(title=&quot;Cases&quot;)), tooltip=[&#39;county:N&#39;, &#39;cases:Q&#39;, &#39;deaths:Q&#39;], color = alt.condition(circle_selection, alt.value(&#39;black&#39;), alt.value(&#39;red&#39;)) ).project( type=&#39;albersUsa&#39; ).properties( width=1000, height=700 ).add_selection( circle_selection ) state = alt.Chart(states).mark_geoshape(fill=&#39;#ededed&#39;, stroke=&#39;white&#39;).encode( ).project( type=&#39;albersUsa&#39; ) state_text = state.mark_text().transform_filter(alt.datum.NAME != &#39;Puerto Rico&#39;).encode( longitude=&#39;lon:Q&#39;, latitude=&#39;lat:Q&#39;, text=&#39;NAME&#39;, ).project( type=&#39;albersUsa&#39; ) . . (state+circles+state_text).configure_view(strokeWidth=0) .",
            "url": "https://richcastro82.github.io/Matrix/2020/06/12/US-case-counts-geospatial.html",
            "relUrl": "/2020/06/12/US-case-counts-geospatial.html",
            "date": " • Jun 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://richcastro82.github.io/Matrix/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://richcastro82.github.io/Matrix/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}